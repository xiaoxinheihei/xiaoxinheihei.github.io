<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>学习使人进步-( ゜- ゜)つロ干杯~</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xiaoxinheihei.github.io/"/>
  <updated>2019-04-15T10:28:04.861Z</updated>
  <id>https://xiaoxinheihei.github.io/</id>
  
  <author>
    <name>Xin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>JVM内存分配策略</title>
    <link href="https://xiaoxinheihei.github.io/2019/03/17/03-Java%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5.html"/>
    <id>https://xiaoxinheihei.github.io/2019/03/17/03-Java内存分配策略.html</id>
    <published>2019-03-17T10:21:08.000Z</published>
    <updated>2019-04-15T10:28:04.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Java-内存分配策略"><a href="#Java-内存分配策略" class="headerlink" title="Java 内存分配策略"></a>Java 内存分配策略</h1><p><img src="/img/内存分配与回收策略.png" alt="内存分配与回收策略.png"></p><blockquote><p><strong>新生代和老年代的 GC 操作</strong></p><ul><li>新生代 GC 操作：Minor GC<ul><li>发生的非常频繁，速度较块。</li></ul></li><li>老年代 GC 操作：Full GC / Major GC<ul><li>经常伴随着至少一次的 Minor GC；</li><li>速度一般比 Minor GC 慢上 10 倍以上。</li></ul></li></ul></blockquote><h2 id="优先在-Eden-区分配"><a href="#优先在-Eden-区分配" class="headerlink" title="优先在 Eden 区分配"></a>优先在 Eden 区分配</h2><ul><li>Eden 空间不够将会触发一次 Minor GC；</li><li>虚拟机参数：<ul><li><code>-Xmx</code>：Java 堆的最大值；</li><li><code>-Xms</code>：Java 堆的最小值；</li><li><code>-Xmn</code>：新生代大小；</li><li><code>-XX:SurvivorRatio=8</code>：Eden 区 / Survivor 区 = 8 : 1</li></ul></li></ul><h2 id="大对象直接进入老年代"><a href="#大对象直接进入老年代" class="headerlink" title="大对象直接进入老年代"></a>大对象直接进入老年代</h2><ul><li><strong>大对象定义：</strong> 需要大量连续内存空间的 Java 对象。例如那种很长的字符串或者数组。</li><li><strong>设置对象直接进入老年代大小限制：</strong><ul><li><code>-XX:PretenureSizeThreshold</code>：单位是字节；<ul><li>只对 Serial 和 ParNew 两款收集器有效。</li></ul></li><li><strong>目的：</strong> 因为新生代采用的是复制算法收集垃圾，大对象直接进入老年代可以避免在 Eden 区和 Survivor 区发生大量的内存复制。</li></ul></li></ul><h2 id="长期存活的对象将进入老年代"><a href="#长期存活的对象将进入老年代" class="headerlink" title="长期存活的对象将进入老年代"></a>长期存活的对象将进入老年代</h2><ul><li><strong>固定对象年龄判定：</strong> 虚拟机给每个对象定义一个年龄计数器，对象每在 Survivor 中熬过一次 Minor GC，年龄 +1，达到 <code>-XX:MaxTenuringThreshold</code> 设定值后，会被晋升到老年代，<code>-XX:MaxTenuringThreshold</code> 默认为 15；</li><li><strong>动态对象年龄判定：</strong> Survivor 中有相同年龄的对象的空间总和大于 Survivor 空间的一半，那么，年龄大于或等于该年龄的对象直接晋升到老年代。</li></ul><h2 id="空间分配担保"><a href="#空间分配担保" class="headerlink" title="空间分配担保"></a>空间分配担保</h2><p>我们知道，新生代采用的是复制算法清理内存，每一次 Minor GC，虚拟机会将 Eden 区和其中一块 Survivor 区的存活对象复制到另一块 Survivor 区，但<strong>当出现大量对象在一次 Minor GC 后仍然存活的情况时，Survivor 区可能容纳不下这么多对象，此时，就需要老年代进行分配担保，即将 Survivor 无法容纳的对象直接进入老年代。</strong></p><p>这么做有一个前提，就是老年代得装得下这么多对象。可是在一次 GC 操作前，虚拟机并不知道到底会有多少对象存活，所以空间分配担保有这样一个判断流程：</p><ul><li>发生 Minor GC 前，虚拟机先检查老年代的最大可用连续空间是否大于新生代所有对象的总空间；<ul><li>如果大于，Minor GC 一定是安全的；</li><li>如果小于，虚拟机会查看 HandlePromotionFailure 参数，看看是否允许担保失败；<ul><li>允许失败：尝试着进行一次 Minor GC；</li><li>不允许失败：进行一次 Full GC；</li></ul></li></ul></li><li>不过 JDK 6 Update 24 后，HandlePromotionFailure 参数就没有用了，规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行 Minor GC，否则将进行 Full GC。</li></ul><h2 id="Metaspace-元空间与-PermGem-永久代"><a href="#Metaspace-元空间与-PermGem-永久代" class="headerlink" title="Metaspace 元空间与 PermGem 永久代"></a>Metaspace 元空间与 PermGem 永久代</h2><p>Java 8 彻底将永久代 (PermGen) 移除出了 HotSpot JVM，将其原有的数据迁移至 Java Heap 或 Metaspace。</p><p><strong>移除 PermGem 的原因：</strong></p><ul><li>PermGen 内存经常会溢出，引发恼人的 java.lang.OutOfMemoryError: PermGen，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM；</li><li>移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。</li></ul><p><strong>移除 PermGem 后，方法区和字符串常量的位置：</strong></p><ul><li>方法区：移至 Metaspace；</li><li>字符串常量：移至 Java Heap。</li></ul><p><strong>Metaspace 的位置：</strong> 本地堆内存(native heap)。</p><p><strong>Metaspace 的优点：</strong> 永久代 OOM 问题将不复存在，因为默认的类的元数据分配只受本地内存大小的限制，也就是说本地内存剩余多少，理论上 Metaspace 就可以有多大；</p><p><strong>JVM参数：</strong></p><ul><li><code>-XX:MetaspaceSize</code>：分配给类元数据空间（以字节计）的初始大小，为估计值。MetaspaceSize的值设置的过大会延长垃圾回收时间。垃圾回收过后，引起下一次垃圾回收的类元数据空间的大小可能会变大。</li><li><code>-XX:MaxMetaspaceSize</code>：分配给类元数据空间的最大值，超过此值就会触发Full GC，取决于系统内存的大小。JVM会动态地改变此值。</li><li><code>-XX:MinMetaspaceFreeRatio</code>：一次GC以后，为了避免增加元数据空间的大小，空闲的类元数据的容量的最小比例，不够就会导致垃圾回收。</li><li><code>-XX:MaxMetaspaceFreeRatio</code>：一次GC以后，为了避免增加元数据空间的大小，空闲的类元数据的容量的最大比例，不够就会导致垃圾回收。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Java-内存分配策略&quot;&gt;&lt;a href=&quot;#Java-内存分配策略&quot; class=&quot;headerlink&quot; title=&quot;Java 内存分配策略&quot;&gt;&lt;/a&gt;Java 内存分配策略&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/img/内存分配与回收策略.png&quot; alt=
      
    
    </summary>
    
      <category term="java" scheme="https://xiaoxinheihei.github.io/categories/java/"/>
    
      <category term="JVM" scheme="https://xiaoxinheihei.github.io/categories/java/JVM/"/>
    
    
      <category term="java" scheme="https://xiaoxinheihei.github.io/tags/java/"/>
    
      <category term="JVM" scheme="https://xiaoxinheihei.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://xiaoxinheihei.github.io/2019/03/16/IoC%E5%AE%B9%E5%99%A8%E6%B5%85%E6%9E%90%E5%8F%8A%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0.html"/>
    <id>https://xiaoxinheihei.github.io/2019/03/16/IoC容器浅析及简单实现.html</id>
    <published>2019-03-16T06:47:18.881Z</published>
    <updated>2019-04-08T13:21:43.909Z</updated>
    
    <content type="html"><![CDATA[<hr><p>layout: spring<br>title: IoC容器浅析及简单实现<br>date: 2018-11-25 14:43:04<br>categories:</p><ul><li>知识总结<br>tags:</li><li>JavaWeb</li><li>Java<br>copyright: true</li></ul><hr><p>Spring IoC 容器是 Spring 框架中最核心的部分，也是初学者难以理解的部分，对于这种关键的设计，简单实现一次能最大限度地加深理解，了解其中思想，对以后的开发也大有裨益。</p><a id="more"></a><h1 id="Spring-IoC-容器浅析及简单实现"><a href="#Spring-IoC-容器浅析及简单实现" class="headerlink" title="Spring IoC 容器浅析及简单实现"></a>Spring IoC 容器浅析及简单实现</h1><h2 id="Spring-IoC-概述"><a href="#Spring-IoC-概述" class="headerlink" title="Spring IoC 概述"></a>Spring IoC 概述</h2><p>原生的 JavaEE 技术中各个模块之间的联系较强，即<code>耦合度较高</code>。</p><p>比如完成一个用户的创建事务，视图层会创建业务逻辑层的对象，再在内部调用对象的方法，各个模块的<code>独立性很差</code>，如果某一模块的代码发生改变，其他模块的改动也会很大。</p><p>而 Spring 框架的核心——IoC（控制反转）很好的解决了这一问题。控制反转，即<code>某一接口具体实现类的选择控制权从调用类中移除，转交给第三方决定</code>，即由 Spring 容器借由 Bean 配置来进行控制。</p><p>可能 IoC 不够开门见山，理解起来较为困难。因此， Martin Fowler 提出了 DI（Dependency Injection，依赖注入）的概念来替代 IoC，即<code>让调用类对某一接口实现类的依赖关系由第三方（容器或写协作类）注入，以移除调用类对某一接口实现类的依赖</code>。</p><p>比如说， 上述例子中，视图层使用业务逻辑层的接口变量，而不需要真正 new 出接口的实现，这样即使接口产生了新的实现或原有实现修改，视图层都能正常运行。</p><p>从注入方法上看，IoC 主要划分为三种类型：构造函数注入、属性注入和接口注入。在开发过程中，一般使用<code>属性注入</code>的方法。</p><p>IoC 不仅可以实现<code>类之间的解耦</code>，还能帮助完成<code>类的初始化与装配工作</code>，让开发者从这些底层实现类的实例化、依赖关系装配等工作中解脱出出来，专注于更有意义的业务逻辑开发工作。</p><h2 id="Spring-IoC-简单实现"><a href="#Spring-IoC-简单实现" class="headerlink" title="Spring IoC 简单实现"></a>Spring IoC 简单实现</h2><p>下面实现了一个IoC容器的核心部分，简单模拟了IoC容器的基本功能。</p><p>下面列举出核心类：</p><p>Student.java</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">/** * @ClassName Student * @Description 学生实体类 * @Author Yixiang Zhao * @Date 2018/9/22 9:19 * @Version 1.0 */public class Student {    private String name;    private String gender;    public void intro() {        System.out.println("My name is " + name + " and I'm " + gender + " .");    }    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }    public String getGender() {        return gender;    }    public void setGender(String gender) {        this.gender = gender;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>StuService.java</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">/** * @ClassName StuService * @Description 学生Service * @Author Yixiang Zhao * @Date 2018/9/22 9:21 * @Version 1.0 */public class StuService {    private Student student;    public Student getStudent() {        return student;    }    public void setStudent(Student student) {        this.student = student;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>beans.xml</p><pre class="line-numbers language-lang-xml"><code class="language-lang-xml"><?xml version="1.0" encoding="UTF-8"?><beans>    <bean id="Student" class="me.seriouszyx.pojo.Student">        <property name="name" value="ZYX"/>        <property name="gender" value="man"/>    </bean>    <bean id="StuService" class="me.seriouszyx.service.StuService">        <property ref="Student"/>    </bean></beans><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面是核心类 ClassPathXMLApplicationContext.java</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">/** * @ClassName ClassPathXMLApplicationContext * @Description ApplicationContext的实现，核心类 * @Author Yixiang Zhao * @Date 2018/9/22 9:40 * @Version 1.0 */public class ClassPathXMLApplicationContext implements ApplicationContext {    private Map map = new HashMap();    public ClassPathXMLApplicationContext(String location) {        try {            Document document = getDocument(location);            XMLParsing(document);        } catch (Exception e) {            e.printStackTrace();        }    }    // 加载资源文件，转换成Document类型    private Document getDocument(String location) throws JDOMException, IOException {        SAXBuilder saxBuilder = new SAXBuilder();        return saxBuilder.build(this.getClass().getClassLoader().getResource(location));    }    private void XMLParsing(Document document) throws Exception {        // 获取XML文件根元素beans        Element beans = document.getRootElement();        // 获取beans下的bean集合        List beanList = beans.getChildren("bean");        // 遍历beans集合        for (Iterator iter = beanList.iterator(); iter.hasNext(); ) {            Element bean = (Element) iter.next();            // 获取bean的属性id和class，id为类的key值，class为类的路径            String id = bean.getAttributeValue("id");            String className = bean.getAttributeValue("class");            // 动态加载该bean代表的类            Object obj = Class.forName(className).newInstance();            // 获得该类的所有方法            Method[] methods = obj.getClass().getDeclaredMethods();            // 获取该节点的所有子节点，子节点存储类的初始化参数            List<Element> properties = bean.getChildren("property");            // 遍历，将初始化参数和类的方法对应，进行类的初始化            for (Element pro : properties) {                for (int i = 0; i < methods.length; i++) {                    String methodName = methods[i].getName();                    if (methodName.startsWith("set")) {                        String classProperty = methodName.substring(3, methodName.length()).toLowerCase();                        if (pro.getAttribute("name") != null) {                            if (classProperty.equals(pro.getAttribute("name").getValue())) {                                methods[i].invoke(obj, pro.getAttribute("value").getValue());                            }                        } else {                            methods[i].invoke(obj, map.get(pro.getAttribute("ref").getValue()));                        }                    }                }            }            // 将初始化完成的对象添加到HashMap中            map.put(id, obj);        }    }    public Object getBean(String name) {        return map.get(name);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后进行测试</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class MyIoCTest {    public static void main(String[] args) {        ApplicationContext context = new ClassPathXMLApplicationContext("beans.xml");        StuService stuService = (StuService) context.getBean("StuService");        stuService.getStudent().intro();    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试成功！</p><pre class="line-numbers language-lang-text"><code class="language-lang-text">My name is ZYX and I'm man .Process finished with exit code 0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>熟悉一个框架最好的方式，就是亲手实现它。这样不仅会深刻地认识到框架的工作原理，以后的使用也会更加得心应手。</p><p>此外，在实现的过程中，又会收获很多东西，就像实现 IoC 容器一样，不仅了解解析 XML 文件的 JDOM 工具，还加深了对 Java 反射的理解。在实际开发中，几乎没有任何地方需要用到反射这一技术，但在框架实现过程中，不懂反射则寸步难行。</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;layout: spring&lt;br&gt;title: IoC容器浅析及简单实现&lt;br&gt;date: 2018-11-25 14:43:04&lt;br&gt;categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;知识总结&lt;br&gt;tags:&lt;/li&gt;
&lt;li&gt;JavaWeb&lt;/li&gt;
&lt;li&gt;Java&lt;br&gt;copyright: true&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Spring IoC 容器是 Spring 框架中最核心的部分，也是初学者难以理解的部分，对于这种关键的设计，简单实现一次能最大限度地加深理解，了解其中思想，对以后的开发也大有裨益。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>JVM垃圾收集</title>
    <link href="https://xiaoxinheihei.github.io/2019/03/10/02-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86(GC).html"/>
    <id>https://xiaoxinheihei.github.io/2019/03/10/02-垃圾收集(GC).html</id>
    <published>2019-03-10T04:21:08.000Z</published>
    <updated>2019-04-15T08:40:48.518Z</updated>
    
    <content type="html"><![CDATA[<h1 id="垃圾收集-GC"><a href="#垃圾收集-GC" class="headerlink" title="垃圾收集 (GC)"></a>垃圾收集 (GC)</h1><p>垃圾收集（Garbage Collection，GC），它的任务是解决以下 3 件问题：</p><ul><li>哪些内存需要回收？</li><li>什么时候回收？</li><li>如何回收？</li></ul><p>其中第一个问题很好回答，在 Java 中，GC 主要发生在 Java 堆和方法区中，对于后两个问题，我们将在之后的内容中进行讨论，并介绍 HotSpot 的 7 个垃圾收集器。</p><h2 id="判断对象的生死"><a href="#判断对象的生死" class="headerlink" title="判断对象的生死"></a>判断对象的生死</h2><p><img src="/img/判断对象的生死.png" alt="判断对象的生死.png"></p><p>什么时候回收对象？当然是这个对象再也不会被用到的时候回收。所以要想解决 “什么时候回收？” 这个问题，我们要先能判断一个对象什么时候什么时候真正的 “死” 掉了，判断对象是否可用主要有以下两种方法。</p><h3 id="判断对象是否可用的算法"><a href="#判断对象是否可用的算法" class="headerlink" title="判断对象是否可用的算法"></a>判断对象是否可用的算法</h3><h4 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h4><ul><li><strong>算法描述：</strong><ul><li>给对象添加一个引用计数器；</li><li>每有一个地方引用它，计数器加 1；</li><li>引用失效时，计数器减 1；</li><li>计数器值为 0 的对象不再可用。</li></ul></li><li><strong>缺点：</strong><ul><li>很难解决循环引用的问题。即 <code>objA.instance = objB; objB.instance = objA;</code>，objA 和 objB 都不会再被访问后，它们仍然相互引用着对方，所以它们的引用计数器不为 0，将永远不能被判为不可用。</li></ul></li></ul><h4 id="可达性分析算法（主流）"><a href="#可达性分析算法（主流）" class="headerlink" title="可达性分析算法（主流）"></a>可达性分析算法（主流）</h4><ul><li><strong>算法描述：</strong><ul><li>从 “GC Root” 对象作为起点开始向下搜索，走过的路径称为引用链（Reference Chain）；</li><li>从 “GC Root” 开始，不可达的对象被判为不可用。</li></ul></li><li><strong>Java 中可作为 “GC Root” 的对象：</strong><ul><li>栈中（本地变量表中的reference）<ul><li>虚拟机栈中，栈帧中的本地变量表引用的对象；</li><li>本地方法栈中，JNI 引用的对象（native方法）；</li></ul></li><li>方法区中<ul><li>类的静态属性引用的对象；</li><li>常量引用的对象；</li></ul></li></ul></li></ul><p>即便如此，一个对象也不是一旦被判为不可达，就立即死去的，宣告一个的死亡需要经过两次标记过程。</p><h3 id="四种引用类型"><a href="#四种引用类型" class="headerlink" title="四种引用类型"></a>四种引用类型</h3><p>JDK 1.2 后，Java 中才有了后 3 种引用的实现。</p><ul><li><strong>强引用：</strong> 像 <code>Object obj = new Object()</code> 这种，只要强引用还存在，垃圾收集器就永远不会回收掉被引用的对象。</li><li><strong>软引用：</strong> 用来引用还存在但非必须的对象。对于软引用对象，在 OOM 前，虚拟机会把这些对象列入回收范围中进行第二次回收，如果这次回收后，内存还是不够用，就 OOM。实现类：<code>SoftReference</code>。</li><li><strong>弱引用：</strong> 被弱引用引用的对象只能生存到下一次垃圾收集前，一旦发生垃圾收集，被弱引用所引用的对象就会被清掉。实现类：<code>WeakReference</code>。</li><li><strong>虚引用：</strong> 幽灵引用，对对象没有半毛钱影响，甚至不能用来取得一个对象的实例。它唯一的用途就是：当被一个虚引用引用的对象被回收时，系统会收到这个对象被回收了的通知。实现类：<code>PhantomReference</code>。</li></ul><h3 id="宣告对象死亡的两次标记过程"><a href="#宣告对象死亡的两次标记过程" class="headerlink" title="宣告对象死亡的两次标记过程"></a>宣告对象死亡的两次标记过程</h3><ul><li>当发现对象不可达后，该对象被第一次标记，并进行是否有必要执行 <code>finalize()</code> 方法的判断；<ul><li>不需要执行：对象没有覆盖 <code>finalize()</code> 方法，或者 <code>finalize()</code> 方法已被执行过（<code>finalize()</code> 只被执行一次）；</li><li>需要执行：将该对象放置在一个队列中，稍后由一个虚拟机自动创建的低优先级线程执行。</li></ul></li><li><code>finalize()</code> 方法是对象逃脱死亡的最后一次机会，不过虚拟机不保证等待 <code>finalize()</code> 方法执行结束，也就是说，虚拟机只触发 <code>finalize()</code> 方法的执行，如果这个方法要执行超久，那么虚拟机并不等待它执行结束，所以最好不要用这个方法。</li><li><code>finalize()</code> 方法能做的，try-finally 都能做，所以忘了这个方法吧！</li></ul><h3 id="方法区的回收"><a href="#方法区的回收" class="headerlink" title="方法区的回收"></a>方法区的回收</h3><p>永久代的 GC 主要回收：<strong>废弃常量</strong> 和 <strong>无用的类</strong>。</p><ul><li>废弃常量：例如一个字符串 “abc”，当没有任何引用指向 “abc” 时，它就是废弃常量了。</li><li>无用的类：同时满足以下 3 个条件的类。<ul><li>该类的所有实例已被回收，Java 堆中不存在该类的任何实例；</li><li>加载该类的 Classloader 已被回收；</li><li>该类的 Class 对象没有被任何地方引用，即无法在任何地方通过反射访问该类的方法。</li></ul></li></ul><h2 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h2><p><img src="/img/垃圾收集算法.png" alt="垃圾收集算法.png"></p><h3 id="基础：标记-清除算法"><a href="#基础：标记-清除算法" class="headerlink" title="基础：标记 - 清除算法"></a>基础：标记 - 清除算法</h3><ul><li><strong>算法描述：</strong><ul><li>先标记出所有需要回收的对象（图中深色区域）；</li><li>标记完后，统一回收所有被标记对象（留下狗啃似的可用内存区域……）。</li></ul></li><li><strong>不足：</strong><ul><li>效率问题：标记和清理两个过程的效率都不高。</li><li>空间碎片问题：标记清除后会产生大量不连续的内存碎片，导致以后为较大的对象分配内存时找不到足够的连续内存，会提前触发另一次 GC。</li></ul></li></ul><p><img src="/img/标记清除GC算法.png" alt="标记清除GC算法.png"></p><h3 id="解决效率问题：复制算法"><a href="#解决效率问题：复制算法" class="headerlink" title="解决效率问题：复制算法"></a>解决效率问题：复制算法</h3><ul><li><p><strong>算法描述：</strong></p><ul><li>将可用内存分为大小相等的两块，每次只使用其中一块；</li><li>当一块内存用完时，将这块内存上还存活的对象复制到另一块内存上去，将这一块内存全部清理掉。</li></ul></li><li><strong>不足：</strong> 可用内存缩小为原来的一半，适合GC过后只有少量对象存活的新生代。</li><li><strong>节省内存的方法：</strong><ul><li>新生代中的对象 98% 都是朝生夕死的，所以不需要按照 1:1 的比例对内存进行划分；</li><li>把内存划分为：<ul><li>1 块比较大的 Eden 区；</li><li>2 块较小的 Survivor 区；</li></ul></li><li>每次使用 Eden 区和 1 块 Survivor 区；</li><li>回收时，将以上 2 部分区域中的存活对象复制到另一块 Survivor 区中，然后将以上两部分区域清空；</li><li>JVM 参数设置：<code>-XX:SurvivorRatio=8</code> 表示 <code>Eden 区大小 / 1 块 Survivor 区大小 = 8</code>。</li></ul></li></ul><p><img src="/img/复制GC算法.png" alt="复制GC算法.png"></p><h3 id="解决空间碎片问题：标记-整理算法"><a href="#解决空间碎片问题：标记-整理算法" class="headerlink" title="解决空间碎片问题：标记 - 整理算法"></a>解决空间碎片问题：标记 - 整理算法</h3><ul><li><strong>算法描述：</strong><ul><li>标记方法与 “标记 - 清除算法” 一样；</li><li>标记完后，将所有存活对象向一端移动，然后直接清理掉边界以外的内存。</li></ul></li><li><strong>不足：</strong> 存在效率问题，适合老年代。</li></ul><p><img src="/img/标记整理GC算法.png" alt="标记整理GC算法.png"></p><h3 id="进化：分代收集算法"><a href="#进化：分代收集算法" class="headerlink" title="进化：分代收集算法"></a>进化：分代收集算法</h3><ul><li><strong>新生代：</strong> GC 过后只有少量对象存活 —— <strong>复制算法</strong></li><li><strong>老年代：</strong> GC 过后对象存活率高 —— <strong>标记 - 整理算法</strong></li></ul><h2 id="HotSpot-中-GC-算法的实现"><a href="#HotSpot-中-GC-算法的实现" class="headerlink" title="HotSpot 中 GC 算法的实现"></a>HotSpot 中 GC 算法的实现</h2><p>通过前两小节对于判断对象生死和垃圾收集算法的介绍，我们已经对虚拟机是进行 GC 的流程有了一个大致的了解。但是，在 HotSpot 虚拟机中，高效的实现这些算法也是一个需要考虑的问题。所以，接下来，我们将研究一下 HotSpot 虚拟机到底是如何高效的实现这些算法的，以及在实现中有哪些需要注意的问题。</p><p><img src="/img/GC的算法实现.png" alt="GC的算法实现.png"></p><p>通过之前的分析，GC 算法的实现流程简单的来说分为以下两步：</p><ol><li>找到死掉的对象；</li><li>把它清了。</li></ol><p>想要找到死掉的对象，我们就要进行可达性分析，也就是从 GC Root 找到引用链的这个操作。</p><p>也就是说，进行可达性分析的第一步，就是要枚举 GC Roots，这就需要虚拟机知道哪些地方存放着对象应用。如果每一次枚举 GC Roots 都需要把整个栈上位置都遍历一遍，那可就费时间了，毕竟并不是所有位置都存放在引用呀。所以为了提高 GC 的效率，HotSpot 使用了一种 OopMap 的数据结构，<strong>OopMap 记录了栈上本地变量到堆上对象的引用关系</strong>，也就是说，GC 的时候就不用遍历整个栈只遍历每个栈的 OopMap 就行了。</p><p>在 OopMap 的帮助下，HotSpot 可以快速准确的完成 GC 枚举了，不过，OopMap 也不是万年不变的，它也是需要被更新的，当内存中的对象间的引用关系发生变化时，就需要改变 OopMap 中的相应内容。可是能导致引用关系发生变化的指令非常之多，如果我们执行完一条指令就改下 OopMap，这 GC 成本实在太高了。</p><p>因此，HotSpot 采用了一种在 “安全点” 更新 OopMap 的方法，安全点的选取既不能让 GC 等待的时间过长，也不能过于频繁增加运行负担，也就是说，我们既要让程序运行一段时间，又不能让这个时间太长。我们知道，JVM 中每条指令执行的是很快的，所以一个超级长的指令流也可能很快就执行完了，所以 <strong>真正会出现 “长时间执行” 的一般是指令的复用，例如：方法调用、循环跳转、异常跳转等</strong>，虚拟机一般会将这些地方设置为安全点更新 OopMap 并判断是否需要进行 GC 操作。</p><p>此外，在进行枚举根节点的这个操作时，为了保证准确性，我们需要在一段时间内 “冻结” 整个应用，即 Stop The World（传说中的 GC 停顿），因为如果在我们分析可达性的过程中，对象的引用关系还在变来变去，那是不可能得到正确的分析结果的。即便是在号称几乎不会发生停顿的 CMS 垃圾收集器中，枚举根节点时也是必须要停顿的。这里就涉及到了一个问题：</p><p><strong>我们让所有线程跑到最近的安全点再停顿下来进行 GC 操作呢？</strong></p><p>主要有以下两种方式：</p><ul><li>抢先式中断：<ul><li>先中断所有线程；</li><li>发现有线程没中断在安全点，恢复它，让它跑到安全点。</li></ul></li><li><strong>主动式中断：</strong> (主要使用)<ul><li>设置一个中断标记；</li><li>每个线程到达安全点时，检查这个中断标记，选择是否中断自己。</li></ul></li></ul><p>除此安全点之外，还有一个叫做 “安全区域” 的东西，一个一直在执行的线程可以自己 “走” 到安全点去，可是一个处于 Sleep 或者 Blocked 状态的线程是没办法自己到达安全点中断自己的，我们总不能让 GC 操作一直等着这些个 ”不执行“ 的线程重新被分配资源吧。对于这种情况，我们要依靠安全区域来解决。</p><p><strong>安全区域是指在一段代码片段之中，引用关系不会发生变化，因此在这个区域中的任意位置开始 GC 都是安全的。</strong></p><p>当线程执行到安全区域时，它会把自己标识为 Safe Region，这样 JVM 发起 GC 时是不会理会这个线程的。当这个线程要离开安全区域时，它会检查系统是否在 GC 中，如果不在，它就继续执行，如果在，它就等 GC 结束再继续执行。</p><p>本小节我们主要讲述 HotSpot 虚拟机是如何发起内存回收的，也就是如何找到死掉的对象，至于如何清掉这些个对象，HotSpot 将其交给了一堆叫做 ”GC 收集器“ 的东西，这东西又有好多种，不同的 GC 收集器的处理方式不同，适用的场景也不同，我们将在下一小节进行详细讲述。</p><h2 id="7-个垃圾收集器"><a href="#7-个垃圾收集器" class="headerlink" title="7 个垃圾收集器"></a>7 个垃圾收集器</h2><p>垃圾收集器就是内存回收操作的具体实现，HotSpot 里足足有 7 种，为啥要弄这么多，因为它们各有各的适用场景。有的属于新生代收集器，有的属于老年代收集器，所以一般是搭配使用的（除了万能的 G1）。关于它们的简单介绍以及分类请见下图。</p><p><img src="/img/垃圾收集器们.png" alt="垃圾收集器们.png"></p><h3 id="Serial-ParNew-搭配-Serial-Old-收集器"><a href="#Serial-ParNew-搭配-Serial-Old-收集器" class="headerlink" title="Serial / ParNew 搭配 Serial Old 收集器"></a>Serial / ParNew 搭配 Serial Old 收集器</h3><p><img src="/img/Serial_ParNew收集器.jpg" alt="Serial_ParNew收集器.jpg"></p><p>Serial 收集器是虚拟机在 Client 模式下的默认新生代收集器，它的优势是简单高效，在单 CPU 模式下很牛。</p><p>ParNew 收集器就是 Serial 收集器的多线程版本，虽然除此之外没什么创新之处，但它却是许多运行在 Server 模式下的虚拟机中的首选新生代收集器，因为除了 Serial 收集器外，只有它能和 CMS 收集器搭配使用。</p><h3 id="Parallel-搭配-Parallel-Scavenge-收集器"><a href="#Parallel-搭配-Parallel-Scavenge-收集器" class="headerlink" title="Parallel 搭配 Parallel Scavenge 收集器"></a>Parallel 搭配 Parallel Scavenge 收集器</h3><p>首先，这俩货肯定是要搭配使用的，不仅仅如此，它俩还贼特别，它们的关注点与其他收集器不同，其他收集器关注于尽可能缩短垃圾收集时用户线程的停顿时间，而 Parallel Scavenge 收集器的目的是达到一个可控的吞吐量。</p><blockquote><p>吞吐量 = 运行用户代码时间 / ( 运行用户代码时间 + 垃圾收集时间 )</p></blockquote><p>因此，Parallel Scavenge 收集器不管是新生代还是老年代都是多个线程同时进行垃圾收集，十分适合于应用在注重吞吐量以及 CPU 资源敏感的场合。</p><p>可调节的虚拟机参数：</p><ul><li><code>-XX:MaxGCPauseMillis</code>：最大 GC 停顿的秒数；</li><li><code>-XX:GCTimeRatio</code>：吞吐量大小，一个 0 ~ 100 的数，<code>最大 GC 时间占总时间的比率 = 1 / (GCTimeRatio + 1)</code>；</li><li><code>-XX:+UseAdaptiveSizePolicy</code>：一个开关参数，打开后就无需手工指定 <code>-Xmn</code>，<code>-XX:SurvivorRatio</code> 等参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，自行调整。</li></ul><h3 id="CMS-收集器"><a href="#CMS-收集器" class="headerlink" title="CMS 收集器"></a>CMS 收集器</h3><p><img src="/img/CMS垃圾收集器.png" alt="CMS垃圾收集器.png"></p><p><img src="/img/CMS收集器运行示意图.jpg" alt="CMS收集器运行示意图.jpg"></p><p><strong>参数设置：</strong></p><ul><li><code>-XX:+UseCMSCompactAtFullCollection</code>：在 CMS 要进行 Full GC 时进行内存碎片整理（默认开启）</li><li><code>-XX:CMSFullGCsBeforeCompaction</code>：在多少次 Full GC 后进行一次空间整理（默认是 0，即每一次 Full GC 后都进行一次空间整理）</li></ul><h3 id="G1-收集器"><a href="#G1-收集器" class="headerlink" title="G1 收集器"></a>G1 收集器</h3><p><img src="/img/G1垃圾收集器.png" alt="G1垃圾收集器.png"></p><p><img src="/img/G1收集器运行示意图.jpg" alt="G1收集器运行示意图.jpg"></p><h2 id="GC-日志解读"><a href="#GC-日志解读" class="headerlink" title="GC 日志解读"></a>GC 日志解读</h2><p><img src="/img/GC日志解读.png" alt="GC日志解读.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;垃圾收集-GC&quot;&gt;&lt;a href=&quot;#垃圾收集-GC&quot; class=&quot;headerlink&quot; title=&quot;垃圾收集 (GC)&quot;&gt;&lt;/a&gt;垃圾收集 (GC)&lt;/h1&gt;&lt;p&gt;垃圾收集（Garbage Collection，GC），它的任务是解决以下 3 件问题：&lt;/
      
    
    </summary>
    
      <category term="java" scheme="https://xiaoxinheihei.github.io/categories/java/"/>
    
      <category term="JVM" scheme="https://xiaoxinheihei.github.io/categories/java/JVM/"/>
    
    
      <category term="java" scheme="https://xiaoxinheihei.github.io/tags/java/"/>
    
      <category term="JVM" scheme="https://xiaoxinheihei.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>Java 容器</title>
    <link href="https://xiaoxinheihei.github.io/2019/03/09/java%E5%AE%B9%E5%99%A8.html"/>
    <id>https://xiaoxinheihei.github.io/2019/03/09/java容器.html</id>
    <published>2019-03-09T04:21:08.000Z</published>
    <updated>2019-04-09T13:27:33.363Z</updated>
    
    <content type="html"><![CDATA[<h1 id="JAVA容器"><a href="#JAVA容器" class="headerlink" title="JAVA容器"></a><strong>JAVA容器</strong></h1><hr><h2 id="一、概览"><a href="#一、概览" class="headerlink" title="一、概览"></a><strong>一、概览</strong></h2><p>容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。</p><h2 id="COLLECTION"><a href="#COLLECTION" class="headerlink" title="COLLECTION"></a>COLLECTION</h2><hr><p><img src="https://i.loli.net/2019/04/09/5cac9d9103af1.png" alt="COLLECTION"></p><h3 id="1-SET"><a href="#1-SET" class="headerlink" title="1. SET"></a>1. SET</h3><ul><li>TreeSet：基于红黑树实现，支持有序性操作，例如根据一个范围查找元素的操作。TreeSet是用来排序的, 可以指定一个顺序, 对象存入之后会按照指定的顺序排列,但是查找效率不如 HashSet，HashSet 查找的时间复杂度为 O(1)，TreeSet 则为 O(logN)；</li><li>HashSet：基于哈希表实现，支持快速查找，但不支持有序性操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的；</li><li>LinkedHashSet：具有 HashSet 的查找效率，且内部使用双向链表维护元素的插入顺序。</li></ul><h3 id="2-LIST"><a href="#2-LIST" class="headerlink" title="2. LIST"></a>2. LIST</h3><ul><li>ArrayList：基于动态数组实现，支持随机访问；</li><li>Vector：和 ArrayList 类似，但它是线程安全的;</li><li>LinkedList：基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。</li></ul><h3 id="3-QUEUUE"><a href="#3-QUEUUE" class="headerlink" title="3. QUEUUE"></a>3. QUEUUE</h3><ul><li>LinkedList：可以用它来实现双向队列；</li><li>PriorityQueue：基于堆结构实现，可以用它来实现优先队列.</li></ul><h2 id="MAP"><a href="#MAP" class="headerlink" title="MAP"></a>MAP</h2><hr><p><img src="https://i.loli.net/2019/04/09/5cac9d503a255.png" alt="map"></p><ul><li>TreeMap：基于红黑树实现;</li><li>HashMap：jdk1.8之前基于数组和链表实现，jdk1.8之后基于数组和链表，当链表的长度大于8时采用红黑树。</li><li>HashTable：和 HashMap 类似，但它是线程安全的，使用Synchronized关键字来保证线程安全，这意味着同一时刻多个线程可以同时写入 HashTable 并且不会导致数据不一致。它是遗留类，不应该去使用它。现在可以使用ConcurrentHashMap来支持线程，ConcurrentHashMap在jdk1.7使用分段锁机制来实现并发，核心类为Segment,jdk1.8使用了CAS操作来支持更高的并发度，在 CAS 操作失败时使用内置锁 synchronized。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;JAVA容器&quot;&gt;&lt;a href=&quot;#JAVA容器&quot; class=&quot;headerlink&quot; title=&quot;JAVA容器&quot;&gt;&lt;/a&gt;&lt;strong&gt;JAVA容器&lt;/strong&gt;&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;一、概览&quot;&gt;&lt;a href=&quot;#一、概览&quot; class
      
    
    </summary>
    
      <category term="java" scheme="https://xiaoxinheihei.github.io/categories/java/"/>
    
    
      <category term="java" scheme="https://xiaoxinheihei.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>java内存区域详解</title>
    <link href="https://xiaoxinheihei.github.io/2019/03/08/java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E8%AF%A6%E8%A7%A3.html"/>
    <id>https://xiaoxinheihei.github.io/2019/03/08/java内存区域详解.html</id>
    <published>2019-03-08T04:21:08.000Z</published>
    <updated>2019-04-09T13:18:39.451Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Java-内存区域详解"><a href="#Java-内存区域详解" class="headerlink" title="Java 内存区域详解"></a><strong>Java 内存区域详解</strong></h1><hr><h2 id="JVM-运行时的数据区域"><a href="#JVM-运行时的数据区域" class="headerlink" title="JVM 运行时的数据区域"></a>JVM 运行时的数据区域</h2><p><img src="https://i.loli.net/2019/04/09/5cac8ada25120.png" alt="JVM 运行时的数据区域"></p><hr><p>总共也就这么 5 个区（直接内存不属于 JVM 运行时数据区的一部分），除了程序计数器其他的地方都有可能出现 OOM (OutOfMemoryError)，其中像是程序计数器和两个栈（Java 虚拟机栈 &amp; 本地方法栈）都是每个线程要有一个的，所以肯定是线程隔离的。而其他 2 个区就是线程共享的了，也就是说，如果有多个线程要同时访问这两个区的数据，是会出现线程安全问题的。接下来，我们将对这些区域进行详细的介绍。</p><h3 id="1-程序计数器"><a href="#1-程序计数器" class="headerlink" title="1. 程序计数器"></a>1. 程序计数器</h3><ul><li>当前线程所执行的字节码的行号指示器，字节码解释器工作就是通过改变这个计数器的值来确定下一条要执行的字节码指令的位置。</li><li>执行Java方法和native方法的区别<ul><li>执行Java方法时：记录虚拟机正在执行的字节码指令的地址</li><li>执行native方法时：无定义</li></ul></li></ul><h3 id="2-Java虚拟机栈"><a href="#2-Java虚拟机栈" class="headerlink" title="2. Java虚拟机栈"></a>2. Java虚拟机栈</h3><ul><li>Java方法执行的内存模型，每个方法执行的过程，就是它所对应的栈帧在虚拟机栈中入栈和出战的过程；</li><li>服务于Java方法；</li><li>可能抛出的异常：<ul><li>OutOfMemoryError (在虚拟机栈可以动态扩展的情况下，扩展时无法申请到足够的内存)；</li><li>StackOverflowError (当线程请求的栈深度大于虚拟机所允许的深度)</li></ul></li><li>虚拟机参数设置：-Xss. </li></ul><h3 id="3-本地方法栈"><a href="#3-本地方法栈" class="headerlink" title="3. 本地方法栈"></a>3. 本地方法栈</h3><ul><li>服务于native方法；</li><li>抛出的异常和Java虚拟机栈一样。</li></ul><h3 id="4-Java堆"><a href="#4-Java堆" class="headerlink" title="4. Java堆"></a>4. Java堆</h3><ul><li>唯一的目的：存放对象实例；</li><li>垃圾收集器管理的主要区域；</li><li>可以处于物理上不连续的内存空间中；</li><li>可能抛出的异常：<ul><li>OutOfMemoryError（堆中没有内存可以分配给新创建的实例，并且堆也无法再继续扩展了）。</li></ul></li><li>虚拟机参数设置：<ul><li>最大值：-Xmx</li><li>最小值：-Xms</li><li>两个参数设置成相同的值可避免堆自动扩展。</li></ul></li></ul><h3 id="5-方法区"><a href="#5-方法区" class="headerlink" title="5. 方法区"></a>5. 方法区</h3><ul><li>存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据；<ul><li>类信息：即 Class 类，如类名、访问修饰符、常量池、字段描述、方法描述等。</li></ul></li><li>垃圾收集行为在此区域很少发生；<ul><li>不过也不能不清理，对于经常动态生成大量 Class 的应用，如 Spring 等，需要特别注意类的回收状况。</li></ul></li><li>运行时常量池也是方法区的一部分；<ul><li>Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池，用于存放编译器生成的各种字面量（就是代码中定义的 static final 常量）和符号引用，这部分信息就存储在运行时常量池中。</li></ul></li><li>可能抛出的异常：<ul><li>OutOfMemoryError（方法区无法满足内存分配需求时）。</li></ul></li></ul><h3 id="6-直接内存"><a href="#6-直接内存" class="headerlink" title="6. 直接内存"></a>6. 直接内存</h3><ul><li>JDK 1.4 的 NIO 类可以使用 native 函数库直接分配堆外内存，这是一种基于通道与缓冲区的 I/O 方式，它在 Java 堆中存储一个 DirectByteBuffer 对象作为堆外内存的引用，这样就可以对堆外内存进行操作了。因为可以避免 Java 堆和 Native 堆之间来回复制数据，在一些场景可以带来显著的性能提高。</li><li>虚拟机参数设置：-XX:MaxDirectMemorySize<ul><li>默认等于 Java 堆最大值，即 -Xmx 指定的值。</li></ul></li><li>将直接内存放在这里讲解的原因是它也可能会出现 OutOfMemoryError；<ul><li>服务器管理员在配置 JVM 参数时，会根据机器的实际内存设置 -Xmx 等信息，但经常会忽略直接内存（默认等于 -Xmx 设置值），这可能会使得各个内存区域的总和大于物理内存限制，从而导致动态扩展时出现 OOM。</li></ul></li></ul><h2 id="HotSpot-虚拟机堆中的对象"><a href="#HotSpot-虚拟机堆中的对象" class="headerlink" title="HotSpot 虚拟机堆中的对象"></a>HotSpot 虚拟机堆中的对象</h2><hr><p>这一小节对JVM对Java堆中的对象的创建、布局和访问的全过程详解。</p><h3 id="对象的创建（遇到一条new指令时）"><a href="#对象的创建（遇到一条new指令时）" class="headerlink" title="对象的创建（遇到一条new指令时）"></a>对象的创建（遇到一条new指令时）</h3><p><img src="https://i.loli.net/2019/04/09/5cac9255d876b.png" alt="对象的创建"></p><hr><p>1.检查这个指令的参数能否在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，先把这个类加载进内存；<br>2.类加载检查通过后，虚拟机将为新对象分配内存，此时已经可以确定存储这个对象所需的内存大小；<br>3.在堆中为新对象分配可用内存；<br>4.将分配到的内存初始化；<br>5.设置对象头中的数据；<br>6.此时，从虚拟机的角度看，对象已经创建好了，但从Java程序的角度看，对象创建才刚刚开始，构造函数还没有执行；<br>7.执行构造函数。<br>第 3 步，在堆中为新对象分配可用内存时，会涉及到以下两个问题：</p><p><strong>如何在堆中为新对象划分可用的内存？</strong></p><ul><li>指针碰撞（内存分配规整）<ul><li>用过的内存放一边，没用过的内存放一边，中间用一个指针分隔；</li><li>分配内存的过程就是将指针向没用过的内存那边移动所需的长度；</li></ul></li><li>空闲列表（内存分配不规整）<ul><li>维护一个列表，记录哪些内存块是可用的；</li></ul></li><li>分配内存时，从列表上选取一块足够大的空间分给对象，并更新列表上的记录；</li></ul><p><strong>如何处理多线程创建对象时，划分内存的指针的同步问题？</strong></p><ul><li>对分配内存空间的动作进行同步处理（CAS）；</li><li>把内存分配动作按照线程划分在不同的空间之中进行；<ul><li>每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）；</li><li>哪个线程要分配内存就在哪个线程的 TLAB 上分配，TLAB 用完需要分配新的 TLAB 时，才需要同步锁定；</li><li>通过 -XX:+/-UseTLAB 参数设定是否使用 TLAB。</li></ul></li></ul><h3 id="对象的内存布局"><a href="#对象的内存布局" class="headerlink" title="对象的内存布局"></a>对象的内存布局</h3><ul><li>对象头：<ul><li>第一部分：存储对象自身运行时的数据，HashCode、GC分代年龄等（Mark Word）、锁状态标志、、偏向线程ID；</li><li>第二部分：类型指针，指向它的类元数据的指针，虚拟机通过这个指针来判断这个对象是哪个类的实例（HotSpot 采用的是直接指针的方式访问对象的），当然类型指针并不一定都在虚拟机中，当有的虚拟机使用句柄来进行对象访问时，此时指针在句柄池中；</li><li>如果是个数组对象，对象头中还有一块用于记录数组长度的数据。</li></ul></li><li>实例数据： <ul><li>默认分配顺序：longs/doubles、ints、shorts/chars、bytes/booleans、oops (Ordinary Object Pointers)，相同宽度的字段会被分配在一起，除了 oops，其他的长度由长到短；</li><li>默认分配顺序下，父类字段会被分配在子类字段前面。</li></ul></li></ul><p>注：HotSpot VM要求对象的起始地址必须是8字节的整数倍，所以不够要补齐。</p><h3 id="对象访问"><a href="#对象访问" class="headerlink" title="对象访问"></a>对象访问</h3><p>Java 程序需要通过虚拟机栈上的 reference 数据来操作堆上的具体对象，reference 数据是一个指向对象的引用，不过如何通过这个引用定位到具体的对象，目前主要有以下两种访问方式：句柄访问和直接指针访问。</p><h4 id="句柄访问"><a href="#句柄访问" class="headerlink" title="句柄访问"></a>句柄访问</h4><p>句柄访问会在 Java 堆中划分一块内存作为句柄池，每一个句柄存放着到对象实例数据和对象类型数据的指针。</p><p>优势：对象移动的时候（这在垃圾回收时十分常见）只需改变句柄池中对象实例数据的指针，不需要修改reference本身。</p><p><img src="https://i.loli.net/2019/04/09/5cac96df545e5.png" alt="句柄访问"></p><h4 id="直接指针访问"><a href="#直接指针访问" class="headerlink" title="直接指针访问"></a>直接指针访问</h4><p>直接指针访问方式在 Java 堆对象的实例数据中存放了一个指向对象类型数据的指针，在 HotSpot 中，这个指针会被存放在对象头中。</p><p>优势：减少了一次指针定位对象实例数据的开销，速度更快。</p><p><img src="https://i.loli.net/2019/04/09/5cac98512434b.png" alt="直接访问"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Java-内存区域详解&quot;&gt;&lt;a href=&quot;#Java-内存区域详解&quot; class=&quot;headerlink&quot; title=&quot;Java 内存区域详解&quot;&gt;&lt;/a&gt;&lt;strong&gt;Java 内存区域详解&lt;/strong&gt;&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;JVM-运行时
      
    
    </summary>
    
      <category term="java" scheme="https://xiaoxinheihei.github.io/categories/java/"/>
    
      <category term="JVM" scheme="https://xiaoxinheihei.github.io/categories/java/JVM/"/>
    
    
      <category term="java" scheme="https://xiaoxinheihei.github.io/tags/java/"/>
    
      <category term="JVM" scheme="https://xiaoxinheihei.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>大数据学习 | 初识 Hadoop</title>
    <link href="https://xiaoxinheihei.github.io/2018/12/25/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0-%E5%88%9D%E8%AF%86Hadoop.html"/>
    <id>https://xiaoxinheihei.github.io/2018/12/25/大数据学习-初识Hadoop.html</id>
    <published>2018-12-25T04:39:24.000Z</published>
    <updated>2019-03-13T05:32:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近想要了解一些前沿技术，不能一门心思眼中只有 web，因为我目前对 Java 语言及其生态相对熟悉，所以在网上搜集了 Hadoop 相关文章，并做了整合。</p><p>本篇文章在于对大数据以及 Hadoop 有一个直观的概念，并上手简单体验。</p><a id="more"></a><h2 id="Hadoop-基础概念"><a href="#Hadoop-基础概念" class="headerlink" title="Hadoop 基础概念"></a>Hadoop 基础概念</h2><p><code>Hadoop</code> 是一个用 Java 实现的开源框架，是一个分布式的解决方案，将大量的信息处理所带来的压力分摊到其他服务器上。</p><p>在了解各个名词之前，我们必须掌握一组概念。</p><h3 id="结构化数据-vs-非结构化数据"><a href="#结构化数据-vs-非结构化数据" class="headerlink" title="结构化数据 vs 非结构化数据"></a>结构化数据 vs 非结构化数据</h3><p><code>结构化数据</code>即行数据，存储在数据库里，可以用二维表结构来表达，例如：名字、电话、家庭住址等。</p><p>常见的结构化数据库为 mysql、sqlserver。</p><p><img src="https://i.loli.net/2018/12/30/5c287655d4f10.jpg" alt="zhhihu1.jpg"></p><p><code>非结构化数据库</code>是指其字段长度可变，并且每个字段的记录又可以由可重复或不可重复的子字段构成的数据库。无法用结构化的数据模型表示，例如：文档、图片、声音、视频等。在大数据时代，对非关系型数据库的需求日益增加，数据库技术相应地进入了“后关系数据库时代”。</p><p>非结构化数据库代表为 HBase、mongodb。</p><p><img src="https://i.loli.net/2018/12/30/5c2876565ece1.jpg" alt="v2-27e5113596ab21aae1d64516ef015100_1200x500.jpg"></p><p>可以大致归纳，结构化数据是先有结构、再有数据；非结构化数据是先有数据、再有结构。</p><p>Hadoop 是大数据存储和计算的开山鼻祖，现在大多数开源大数据框架都依赖 Hadoop 或者与它能很好地兼容，下面开始讲述 Hadoop 的相关概念。</p><h3 id="Hadoop-1-0-vs-Hadoop-2-0"><a href="#Hadoop-1-0-vs-Hadoop-2-0" class="headerlink" title="Hadoop 1.0 vs Hadoop 2.0"></a>Hadoop 1.0 vs Hadoop 2.0</h3><p><img src="https://i.loli.net/2018/12/27/5c242519227c9.png" alt="Hadoop-1-vs-Hadoop-2-Architecture.png"></p><h3 id="HDFS-和-MapReduce"><a href="#HDFS-和-MapReduce" class="headerlink" title="HDFS 和 MapReduce"></a>HDFS 和 MapReduce</h3><p>Hadoop 为解决<code>存储</code>和<code>分析</code>大量数据而生，所以这两部分也是 Hadoop 的狭义说法（广义指 Hadoop 生态）。HDFS 提供了一种安全可靠的分布式文件存储系统，MapReduce 提供了基于批处理模式的数据分析框架。</p><p><code>HDFS</code>（Hadoop Distributed File System）的设计本质上是为了大量的数据能横跨很多台机器，但是你看到的是一个文件系统而不是很多个文件系统。就好比访问 <code>/hdfs/tmp/file1</code> 的数据，引用的是一个文件路径，但是实际数据可能分布在很多机器上，当然 HDFS 为你管理这些数据，用户并不需要了解它如何管理。</p><p>关于 <code>MapReduce</code>，这里通过一个具体模型来解释。</p><p>考虑如果你要统计一个巨大的文本文件存储在类似 HDFS 上，你想要知道这个文本里各个词的出现频率。你启动了一个 MapReduce 程序。Map 阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似（hello, 12100次），（world，15214次）等等这样的 Pair（我这里把 Map 和 Combine 放在一起说以便简化）；这几百台机器各自都产生了如上的集合，然后又有几百台机器启动 Reduce 处理。Reducer 机器 A 将从 Mapper 机器收到所有以 A 开头的统计结果，机器 B 将收到 B 开头的词汇统计结果（当然实际上不会真的以字母开头做依据，而是用函数产生 Hash 值以避免数据串化。因为类似 X 开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊）。然后这些Reducer将再次汇总，（hello，12100）＋（hello，12311）＋（hello，345881）= （hello，370292）。每个 Reducer 都如上处理，你就得到了整个文件的词频结果。</p><p>这就是一个简单的 <code>WordCount</code> 的例子，Map+Reduce 这种简单模型暴力好用，不过很笨重，关于更高效的解决方法，以后再详细描述。</p><h3 id="Hadoop-构建模块"><a href="#Hadoop-构建模块" class="headerlink" title="Hadoop 构建模块"></a>Hadoop 构建模块</h3><p>下面从底层实现的角度解释 HDFS 和 MapReduce 的一些概念。</p><p><code>NameNode</code> 是 Hadoop 守护进程中最重要的一个。NameNode 位于 HDFS 的主端，指导 DataNode 执行底层的 IO 任务。NameNode 的运行消耗大量内存和 IO 资源，所以 NameNode 服务器不会同时是 DataNode 或 TaskTracker。</p><p>NameNode 和 <code>DataNode</code> 为主/从结构（Master/Slave）。每一个集群上的从节点都会驻留一个 DataNode 守护进程，来执行分布式文件系统的繁重工作，将 HDFS 数据块读取或者写入到本地文件系统的实际文件中。当希望对 HDFS 文件进行读写时，文件被分割为多个块，由NameNode 告知客户端每个数据块驻留在那个 DataNode。客户端直接与 DataNode 守护进程通信，来处理与数据块相对应的本地文件。</p><p><code>SNN</code>（Scondary NameNode）是监测 HDFS 集群状态的辅助守护进程。SNN 快照有助于加少停机的时间并降低数据丢失的风险。</p><p><code>JobTracker</code> 守护进程是应用程序和 Hadoop 之间的纽带。一旦提交代码到集群上，JobTracker 就会确定执行计划，包括决定处理哪些文件，为不同的任务分配节点以及监控所有任务的运行。如果任务失败，JobTracker 将自动重启任务，但所分配的节点可能会不同，同时受到预定义的重试次数限制。每一个Hadoop集群只有一个JobTracker守护进程，它通常运行在服务器集群的主节点上。</p><p>JobTracker 和 <code>TaskTracker</code> 也是主/从结构。JobTracker 作为主节点，监测 MapReduce 作业的整个执行过程，同时，TaskTracker 管理各个任务在每个从节点上的执行情况。TaskTracker 的一个职责就是负责持续不断地与 JobTracker 通讯。如果 JobTracker 在指定的时间内没有收到来自 TaskTracker 的心跳，它会假定 TaskTracker 已经崩溃了，进而重新提交相应的任务到集群的其他节点中。</p><h2 id="尝试使用-Hadoop"><a href="#尝试使用-Hadoop" class="headerlink" title="尝试使用 Hadoop"></a>尝试使用 Hadoop</h2><p><code>Hadoop 安装</code>可以直接看官方文档，或是 Google 一些不错的教程，比如 <a href="https://chu888chu888.gitbooks.io/hadoopstudy/content/Content/4/chapter0401.html" target="_blank" rel="noopener">Hadoop 的安装</a>、<a href="https://www.jianshu.com/p/de7eb61c983a" target="_blank" rel="noopener">Mac 系统安装Hadoop 2.7.3</a>。</p><p>按照操作配置 Hadoop 并成功运行，访问<code>localhost:50070</code> 和 <code>localhost:8088</code> 分别显示一下页面。</p><p><img src="https://i.loli.net/2018/12/27/5c2457210b58d.png" alt="90496E3D-A8FB-41CE-9FF0-3B962184AFAE.png"></p><p><img src="https://i.loli.net/2018/12/27/5c2457214a2e2.png" alt="1CBC323A-55DC-40AC-B258-3725DD0D4350.png"></p><p>运行<code>伪分布式</code>样例：</p><p><img src="https://i.loli.net/2018/12/27/5c24700c97c3a.png" alt="31D3E6A6-5864-4C6E-865E-AE576A64E647.png"></p><h3 id="HDFS-目录-文件操作命令"><a href="#HDFS-目录-文件操作命令" class="headerlink" title="HDFS 目录/文件操作命令"></a>HDFS 目录/文件操作命令</h3><p>HDFS 是一种文件系统，它可以将一个很大的数据集存储为一个文件，而大多数其他文件系统无力于这一点。Hadoop 也为它提供了一种与 Linux 命令类似的命令行工具，我们可以进行一些简单的操作。</p><p>Hadoop 的<code>文件命令</code>采取的形式为</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">hadoop fs -cmd <args><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>其中 cmd 为具体的文件命令，通常与 UNIX 对应的命令名相同，比如：</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">hadoop fs -lshadoop fs -mkdir /user/seriouszyxhadoop fs -lsr /hadoop fs -rm example.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>还有一些本地文件系统和 HDFS 交互的命令，也经常使用到。</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">hadoop fs -put example.txt /user/seriouszyxhadoop fs -get example.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="Hadoop-构建模块的原理"><a href="#Hadoop-构建模块的原理" class="headerlink" title="Hadoop 构建模块的原理"></a>Hadoop 构建模块的原理</h2><h3 id="MapReduce-如何分而治之"><a href="#MapReduce-如何分而治之" class="headerlink" title="MapReduce 如何分而治之"></a>MapReduce 如何分而治之</h3><p>MapReduce 是用来处理大规模数据的一个并行编程框架，采用了对数据“分而治之”的方法。</p><p><img src="https://i.loli.net/2018/12/30/5c28765631bc8.png" alt="40658-2de7c5066daf7ab1.png"></p><p>MapReduce 是一个离线计算框架，它将计算分为两个阶段，Map（并行处理输入数据）和 Reduce（对 Map 结果汇总）。其中 Map 和 Reduce 函数提供了两个高层接口，由用户去编程实现。</p><p>Map 的一般处理逻辑为：<strong>(k1;v1) ——&gt;map 处理——&gt;[(k2;v2)]</strong></p><p>Reduce 函数的一般处理逻辑是：<strong>(k2;[v2])——&gt;reduce 处理——&gt;[(k3;v3)]</strong></p><p>可以看出 map 处理的输出与 reduce 的输入并不完全相同，这是因为输入参数在进入 reduce 前，一般会将相同键 k2 下的所有值 v2 合并到一个集合中处理：<strong>[(k2;v2)]—-&gt;(k2;[v2])</strong>，这个过程叫 Combiner。</p><p>在经过 Map 和 Reduce 的抽象后，并行结构模型就变成了下面这样：</p><p><img src="https://i.loli.net/2018/12/30/5c28765664bab.png" alt="40658-df82b7a1775fac75.png"></p><p>上图中可以发现，中间有一个同步障（Barrier），其作用是等所有的 map 节点处理完后才进入 reduce，并且这个阶段同时进行数据加工整理过程（Aggregation &amp; Shuffle），以便 reduce 节点可以完全基于本节点上的数据计算最终结果。</p><p>不过这仍然不是完整的 MapReduce 模型，在上述框架图中，还少了两个步骤 Combiner 和 Partitioner。</p><p><img src="https://i.loli.net/2018/12/30/5c287656a01e5.png" alt="40658-39cc7b851195657c.png"></p><p>上述图以<code>词频统计（WordCount）</code>为例。</p><p><strong>Combiner</strong> 用来对中间结果数据网络传输进行优化，比如 map 处理完输出很多键值对后，某些键值对的键是相同的，Combiner 就会将相同的键合并，比如有两个键值对的键相同（good，1）和（good，2），便可以合成(good,3)。</p><p>这样，可以减少需要传输的中间结果数据量，打倒网络数据传输优化，因为 map 传给 reduce 是通过网络来传的。</p><p><strong>Partitioner</strong> 负责对中间结果进行分区处理。比如词频统计，将所有主键相同的键值对传输给同一个 Reduce 节点，以便 Reduce 节点不需要访问其他 Reduce 节点的情况下，一次性对分过来的中间结果进行处理。</p><h3 id="副本机制"><a href="#副本机制" class="headerlink" title="副本机制"></a>副本机制</h3><p>我们再说回 HDFS 诞生的原因，hdfs 由 Google 最先研发，其需求是单独一台计算机所能存储的空间是有限的，而随着计算机存储空间的加大，其价格是呈几何倍的增长。而 hdfs 架构在相对廉价的计算机上，以分布式的方式，这样想要扩大空间之遥增加集群的数量就可以了。</p><p>大量相对廉价的计算机，那么说明<strong>宕机</strong>就是一种必然事件，我们需要让数据避免丢失，就只用采取冗余数据存储，而具体的实现的就是<code>副本机制</code>。</p><p><img src="http://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.png" alt></p><p>hdfs 主要使用<code>三副本机制</code></p><ul><li>第一副本：如果上传节点是 DN，则上传该节点；如果上传节点是 NN，则随机选择 DN</li><li>第二副本：放置在不同机架的 DN 上</li><li>第三副本：放置在与第二副本相同机架的不同 DN 上</li></ul><p>除了极大程度地避免宕机所造成的数据损失，副本机制还可以在数据读取时进行数据校验。</p><h3 id="NameNode-在做些什么"><a href="#NameNode-在做些什么" class="headerlink" title="NameNode 在做些什么"></a>NameNode 在做些什么</h3><p>在 Hadoop 1.0 时代，Hadoop 两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，其中以 NameNode 最为严重。因为 <code>NameNode 保存了整个 HDFS 的元数据信息</code>，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。</p><p>这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。</p><p>所幸的是，在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。</p><p><img src="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/img001.png" alt></p><p>从上图中我们可以看到，有两台 NameNode——Active NameNode 和 Standby NameNode，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。</p><h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p><code>Yarn</code> 是 Hadoop 集群的新一代资源管理系统。Hadoop 2.0 对 MapReduce 框架做了彻底的设计重构，我们称 Hadoop 2.0 中的 MapReduce 为 MRv2 或者 Yarn。</p><p><img src="https://i.loli.net/2018/12/30/5c28775f7eaa5.jpg" alt="20151029092726524 (2).jpg"></p><p>在 Hadoop 2.x 中，Yarn 把 job 的概念换成了 <code>application</code>，因为运行的应用不只是 MapReduce 了，还可能是其他应用，如一个 DAG（有向无环图 Directed Acyclic Graph，例如 Storm 应用）。</p><p>Yarn 另一个目标是扩展 Hadoop，使得它不仅仅可以支持 MapReduce 计算，还能很方便地管理诸如 Hive、Pig、Hbase、Spark/Shark 等应用。</p><p>这种新的架构设计能够使得各种类型的应用运行在 Hadoop 上面，并通过 Yarn 从系统层面进行统一的管理，也就是说，有了 Yarn，<strong>各种应用就可以互不干扰的运行在同一个 Hadoop 系统中</strong>，共享整个集群资源。</p><h3 id="ResourceManager-在做些什么"><a href="#ResourceManager-在做些什么" class="headerlink" title="ResourceManager 在做些什么"></a>ResourceManager 在做些什么</h3><p>刚刚提到的 Yarn 也采用了 Master/Slave 结构，其中 Master 为 <strong>ResourceManager</strong>，负责整个集群的资源管理与调度；Slave 实现为 <strong>NodeManager</strong>，负责单个节点的组员管理与任务启动。 </p><p>ResourceManager 是整个 Yarn 集群中最重要的组件之一，它的功能较多，包括 ApplicationMaster 管理（启动、停止等）、NodeManager 管理、Application 管理、状态机管理等。</p><p>ResourceManager 主要完成以下几个功能：</p><ul><li>与客户端交互，处理来自客户端的请求</li><li>启动和管理 ApplicationMaster，并在它失败时重新启动它</li><li>管理 NodeManager，接受来自 NodeManager 的资源管理汇报信息，并向 NodeManager 下达管理命令或把信息按照一定的策略分配给各个应用程序（ApplicationManager）等</li><li><strong>资源管理与调度，接受来自 ApplicationMaster 的资源申请请求，并为之分配资源（核心）</strong></li></ul><p>在 Master/Slave 架构中，ResourceManager 同样存在单点故障（高可用问题，High Availability）问题。为了解决它，通常采用热备方案，即集群中存在一个对外服务的 Active Master 和若干个处于就绪状态的 Standy Master，一旦 Active Master 出现故<br>障，立即采用一定的侧率选取某个 Standy Master 转换为 Active Master 以正常对外提供服务。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了 Hadoop 的相关概念，包括量大核心部件 HDFS 和 MapReduce，并对其进行了进一步剖析，Hadoop 2.0 的 Yarn 的简单介绍，以及一些问题的解决方法（如 HA）。</p><p>也通过配置第一次在本机上配置了 Hadoop 的运行环境，运行了伪分布式样例。</p><p>接下来会结合一个具体问题深入理解 Hadoop 的方方面面。</p><p><br></p><blockquote><p>  References:<br>   <a href="https://chu888chu888.gitbooks.io/hadoopstudy/content/" target="_blank" rel="noopener">大数据学习笔记</a><br>   <a href="https://zhuanlan.zhihu.com/p/26545566" target="_blank" rel="noopener">一文读懂大数据平台——写给大数据开发初学者的话!</a><br>  <a href="https://www.jianshu.com/p/ed6b35f52e3c" target="_blank" rel="noopener">Hadoop HDFS和MapReduce</a><br>  <a href="http://pangjiuzala.github.io/2015/08/03/HDFS%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/" target="_blank" rel="noopener">HDFS文件操作</a><br>  <a href="https://www.jianshu.com/p/35be7bdca902" target="_blank" rel="noopener">hadoop笔记4—MapReduce框架</a><br>  <a href="https://blog.csdn.net/suifeng3051/article/details/49486927" target="_blank" rel="noopener">Hadoop Yarn详解</a><br>  <a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html" target="_blank" rel="noopener">Hadoop NameNode 高可用 (High Availability) 实现解析</a><br><a href="https://blog.csdn.net/zhangzhebjut/article/details/37730065" target="_blank" rel="noopener">Hadoop -YARN ResourceManager 剖析</a></p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近想要了解一些前沿技术，不能一门心思眼中只有 web，因为我目前对 Java 语言及其生态相对熟悉，所以在网上搜集了 Hadoop 相关文章，并做了整合。&lt;/p&gt;
&lt;p&gt;本篇文章在于对大数据以及 Hadoop 有一个直观的概念，并上手简单体验。&lt;/p&gt;
    
    </summary>
    
      <category term="知识总结" scheme="https://xiaoxinheihei.github.io/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="大数据" scheme="https://xiaoxinheihei.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hadoop" scheme="https://xiaoxinheihei.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>[总结|展望] 世界不会因为你的无知而停下脚步</title>
    <link href="https://xiaoxinheihei.github.io/2018/12/10/123.html"/>
    <id>https://xiaoxinheihei.github.io/2018/12/10/123.html</id>
    <published>2018-12-10T14:18:05.000Z</published>
    <updated>2019-03-18T06:52:34.639Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Be the greatest, or nothing</strong>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Be the greatest, or nothing&lt;/strong&gt;。&lt;/p&gt;

      
    
    </summary>
    
      <category term="人生苦旅" scheme="https://xiaoxinheihei.github.io/categories/%E4%BA%BA%E7%94%9F%E8%8B%A6%E6%97%85/"/>
    
    
      <category term="大学生活" scheme="https://xiaoxinheihei.github.io/tags/%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%B4%BB/"/>
    
      <category term="总结" scheme="https://xiaoxinheihei.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
</feed>
