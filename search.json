[{"url":"%2F2019%2F03%2F16%2FIoC%E5%AE%B9%E5%99%A8%E6%B5%85%E6%9E%90%E5%8F%8A%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0.html","content":"---\rlayout: spring\rtitle: IoC容器浅析及简单实现\rdate: 2018-11-25 14:43:04\rcategories:\r- 知识总结\rtags:\r- JavaWeb\r- Java\rcopyright: true\r---\r\rSpring IoC 容器是 Spring 框架中最核心的部分，也是初学者难以理解的部分，对于这种关键的设计，简单实现一次能最大限度地加深理解，了解其中思想，对以后的开发也大有裨益。\r\r\r\r<!-- more -->\r\r#   Spring IoC 容器浅析及简单实现\r\r\r##\tSpring IoC 概述\r\r原生的 JavaEE 技术中各个模块之间的联系较强，即`耦合度较高`。\r\r比如完成一个用户的创建事务，视图层会创建业务逻辑层的对象，再在内部调用对象的方法，各个模块的`独立性很差`，如果某一模块的代码发生改变，其他模块的改动也会很大。\r\r而 Spring 框架的核心——IoC（控制反转）很好的解决了这一问题。控制反转，即`某一接口具体实现类的选择控制权从调用类中移除，转交给第三方决定`，即由 Spring 容器借由 Bean 配置来进行控制。\r\r可能 IoC 不够开门见山，理解起来较为困难。因此， Martin Fowler 提出了 DI（Dependency Injection，依赖注入）的概念来替代 IoC，即`让调用类对某一接口实现类的依赖关系由第三方（容器或写协作类）注入，以移除调用类对某一接口实现类的依赖`。\r\r比如说， 上述例子中，视图层使用业务逻辑层的接口变量，而不需要真正 new 出接口的实现，这样即使接口产生了新的实现或原有实现修改，视图层都能正常运行。\r\r从注入方法上看，IoC 主要划分为三种类型：构造函数注入、属性注入和接口注入。在开发过程中，一般使用`属性注入`的方法。\r\rIoC 不仅可以实现`类之间的解耦`，还能帮助完成`类的初始化与装配工作`，让开发者从这些底层实现类的实例化、依赖关系装配等工作中解脱出出来，专注于更有意义的业务逻辑开发工作。\r\r##\tSpring IoC 简单实现\r\r下面实现了一个IoC容器的核心部分，简单模拟了IoC容器的基本功能。\r\r\r下面列举出核心类：\r\rStudent.java\r\r```java\r/**\r * @ClassName Student\r * @Description 学生实体类\r * @Author Yixiang Zhao\r * @Date 2018/9/22 9:19\r * @Version 1.0\r */\rpublic class Student {\r\r    private String name;\r\r    private String gender;\r\r    public void intro() {\r        System.out.println(\"My name is \" + name + \" and I'm \" + gender + \" .\");\r    }\r\r    public String getName() {\r        return name;\r    }\r\r    public void setName(String name) {\r        this.name = name;\r    }\r\r    public String getGender() {\r        return gender;\r    }\r\r    public void setGender(String gender) {\r        this.gender = gender;\r    }\r}\r```\r\rStuService.java\r\r```java\r/**\r * @ClassName StuService\r * @Description 学生Service\r * @Author Yixiang Zhao\r * @Date 2018/9/22 9:21\r * @Version 1.0\r */\rpublic class StuService {\r\r    private Student student;\r\r    public Student getStudent() {\r        return student;\r    }\r\r    public void setStudent(Student student) {\r        this.student = student;\r    }\r}\r```\r\rbeans.xml\r\r```xml\r<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\r<beans>\r    <bean id=\"Student\" class=\"me.seriouszyx.pojo.Student\">\r        <property name=\"name\" value=\"ZYX\"/>\r        <property name=\"gender\" value=\"man\"/>\r    </bean>\r\r    <bean id=\"StuService\" class=\"me.seriouszyx.service.StuService\">\r        <property ref=\"Student\"/>\r    </bean>\r</beans>\r```\r\r下面是核心类 ClassPathXMLApplicationContext.java\r\r```java\r\r/**\r * @ClassName ClassPathXMLApplicationContext\r * @Description ApplicationContext的实现，核心类\r * @Author Yixiang Zhao\r * @Date 2018/9/22 9:40\r * @Version 1.0\r */\rpublic class ClassPathXMLApplicationContext implements ApplicationContext {\r\r    private Map map = new HashMap();\r\r    public ClassPathXMLApplicationContext(String location) {\r        try {\r            Document document = getDocument(location);\r            XMLParsing(document);\r        } catch (Exception e) {\r            e.printStackTrace();\r        }\r    }\r\r    // 加载资源文件，转换成Document类型\r    private Document getDocument(String location) throws JDOMException, IOException {\r        SAXBuilder saxBuilder = new SAXBuilder();\r        return saxBuilder.build(this.getClass().getClassLoader().getResource(location));\r    }\r\r    private void XMLParsing(Document document) throws Exception {\r        // 获取XML文件根元素beans\r        Element beans = document.getRootElement();\r        // 获取beans下的bean集合\r        List beanList = beans.getChildren(\"bean\");\r        // 遍历beans集合\r        for (Iterator iter = beanList.iterator(); iter.hasNext(); ) {\r            Element bean = (Element) iter.next();\r            // 获取bean的属性id和class，id为类的key值，class为类的路径\r            String id = bean.getAttributeValue(\"id\");\r            String className = bean.getAttributeValue(\"class\");\r            // 动态加载该bean代表的类\r            Object obj = Class.forName(className).newInstance();\r            // 获得该类的所有方法\r            Method[] methods = obj.getClass().getDeclaredMethods();\r            // 获取该节点的所有子节点，子节点存储类的初始化参数\r            List<Element> properties = bean.getChildren(\"property\");\r            // 遍历，将初始化参数和类的方法对应，进行类的初始化\r            for (Element pro : properties) {\r                for (int i = 0; i < methods.length; i++) {\r                    String methodName = methods[i].getName();\r                    if (methodName.startsWith(\"set\")) {\r                        String classProperty = methodName.substring(3, methodName.length()).toLowerCase();\r                        if (pro.getAttribute(\"name\") != null) {\r                            if (classProperty.equals(pro.getAttribute(\"name\").getValue())) {\r                                methods[i].invoke(obj, pro.getAttribute(\"value\").getValue());\r                            }\r                        } else {\r                            methods[i].invoke(obj, map.get(pro.getAttribute(\"ref\").getValue()));\r                        }\r                    }\r                }\r            }\r            // 将初始化完成的对象添加到HashMap中\r            map.put(id, obj);\r        }\r    }\r\r    public Object getBean(String name) {\r        return map.get(name);\r    }\r\r}\r```\r\r最后进行测试\r\r```java\rpublic class MyIoCTest {\r    public static void main(String[] args) {\r        ApplicationContext context = new ClassPathXMLApplicationContext(\"beans.xml\");\r        StuService stuService = (StuService) context.getBean(\"StuService\");\r        stuService.getStudent().intro();\r    }\r}\r```\r\r测试成功！\r\r```text\rMy name is ZYX and I'm man .\r\rProcess finished with exit code 0\r```\r\r\r\r##\t总结\r\r熟悉一个框架最好的方式，就是亲手实现它。这样不仅会深刻地认识到框架的工作原理，以后的使用也会更加得心应手。\r\r此外，在实现的过程中，又会收获很多东西，就像实现 IoC 容器一样，不仅了解解析 XML 文件的 JDOM 工具，还加深了对 Java 反射的理解。在实际开发中，几乎没有任何地方需要用到反射这一技术，但在框架实现过程中，不懂反射则寸步难行。\r\r\r"},{"title":"Java 容器","url":"%2F2019%2F03%2F09%2Fjava%E5%AE%B9%E5%99%A8.html","content":"\n# **JAVA容器**\n\n------\n\n##**一、概览**\n\n容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。\n\n\n\n\n\n\n\n## COLLECTION\n----------\n\n![COLLECTION](https://i.loli.net/2019/04/09/5cac9d9103af1.png)\n  \n### 1. SET\n\n- TreeSet：基于红黑树实现，支持有序性操作，例如根据一个范围查找元素的操作。TreeSet是用来排序的, 可以指定一个顺序, 对象存入之后会按照指定的顺序排列,但是查找效率不如 HashSet，HashSet 查找的时间复杂度为 O(1)，TreeSet 则为 O(logN)；\n- HashSet：基于哈希表实现，支持快速查找，但不支持有序性操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的；\n- LinkedHashSet：具有 HashSet 的查找效率，且内部使用双向链表维护元素的插入顺序。\n\n\n\n\n### 2. LIST\n\n- ArrayList：基于动态数组实现，支持随机访问；\n- Vector：和 ArrayList 类似，但它是线程安全的;\n- LinkedList：基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。\n\n### 3. QUEUUE \n- LinkedList：可以用它来实现双向队列；\n- PriorityQueue：基于堆结构实现，可以用它来实现优先队列.\n\n\n\n\n\n## MAP\n----------\n\n\n\n![map](https://i.loli.net/2019/04/09/5cac9d503a255.png)\n\n\n\n- TreeMap：基于红黑树实现;\n- HashMap：jdk1.8之前基于数组和链表实现，jdk1.8之后基于数组和链表，当链表的长度大于8时采用红黑树。\n- HashTable：和 HashMap 类似，但它是线程安全的，使用Synchronized关键字来保证线程安全，这意味着同一时刻多个线程可以同时写入 HashTable 并且不会导致数据不一致。它是遗留类，不应该去使用它。现在可以使用ConcurrentHashMap来支持线程，ConcurrentHashMap在jdk1.7使用分段锁机制来实现并发，核心类为Segment,jdk1.8使用了CAS操作来支持更高的并发度，在 CAS 操作失败时使用内置锁 synchronized。\n\n\n","tags":["java"],"categories":["java"]},{"title":"java内存区域详解","url":"%2F2019%2F03%2F08%2Fjava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E8%AF%A6%E8%A7%A3.html","content":"\n# **Java 内存区域详解**\n------\n\n\n##JVM 运行时的数据区域\n\n\n![JVM 运行时的数据区域](https://i.loli.net/2019/04/09/5cac8ada25120.png)\n\n\n------\n\n总共也就这么 5 个区（直接内存不属于 JVM 运行时数据区的一部分），除了程序计数器其他的地方都有可能出现 OOM (OutOfMemoryError)，其中像是程序计数器和两个栈（Java 虚拟机栈 & 本地方法栈）都是每个线程要有一个的，所以肯定是线程隔离的。而其他 2 个区就是线程共享的了，也就是说，如果有多个线程要同时访问这两个区的数据，是会出现线程安全问题的。接下来，我们将对这些区域进行详细的介绍。\n\n\n\n\n\n### 1. 程序计数器\n\n- 当前线程所执行的字节码的行号指示器，字节码解释器工作就是通过改变这个计数器的值来确定下一条要执行的字节码指令的位置。\n- 执行Java方法和native方法的区别\n  - 执行Java方法时：记录虚拟机正在执行的字节码指令的地址\n  - 执行native方法时：无定义\n  \n### 2. Java虚拟机栈\n\n- Java方法执行的内存模型，每个方法执行的过程，就是它所对应的栈帧在虚拟机栈中入栈和出战的过程；\n- 服务于Java方法；\n- 可能抛出的异常：\n - OutOfMemoryError (在虚拟机栈可以动态扩展的情况下，扩展时无法申请到足够的内存)；\n - StackOverflowError (当线程请求的栈深度大于虚拟机所允许的深度)\n- 虚拟机参数设置：-Xss. \n\n\n\n### 3. 本地方法栈\n\n- 服务于native方法；\n- 抛出的异常和Java虚拟机栈一样。\n\n### 4. Java堆\n- 唯一的目的：存放对象实例；\n- 垃圾收集器管理的主要区域；\n- 可以处于物理上不连续的内存空间中；\n- 可能抛出的异常：\n   - OutOfMemoryError（堆中没有内存可以分配给新创建的实例，并且堆也无法再继续扩展了）。\n- 虚拟机参数设置：\n - 最大值：-Xmx\n - 最小值：-Xms\n - 两个参数设置成相同的值可避免堆自动扩展。\n\n\n### 5. 方法区\n\n- 存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据；\n - 类信息：即 Class 类，如类名、访问修饰符、常量池、字段描述、方法描述等。\n- 垃圾收集行为在此区域很少发生；\n - 不过也不能不清理，对于经常动态生成大量 Class 的应用，如 Spring 等，需要特别注意类的回收状况。\n- 运行时常量池也是方法区的一部分；\n - Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项是常量池，用于存放编译器生成的各种字面量（就是代码中定义的 static final 常量）和符号引用，这部分信息就存储在运行时常量池中。\n- 可能抛出的异常：\n - OutOfMemoryError（方法区无法满足内存分配需求时）。\n \n\n\n\n### 6. 直接内存\n\n- JDK 1.4 的 NIO 类可以使用 native 函数库直接分配堆外内存，这是一种基于通道与缓冲区的 I/O 方式，它在 Java 堆中存储一个 DirectByteBuffer 对象作为堆外内存的引用，这样就可以对堆外内存进行操作了。因为可以避免 Java 堆和 Native 堆之间来回复制数据，在一些场景可以带来显著的性能提高。\n- 虚拟机参数设置：-XX:MaxDirectMemorySize\n - 默认等于 Java 堆最大值，即 -Xmx 指定的值。\n- 将直接内存放在这里讲解的原因是它也可能会出现 OutOfMemoryError；\n - 服务器管理员在配置 JVM 参数时，会根据机器的实际内存设置 -Xmx 等信息，但经常会忽略直接内存（默认等于 -Xmx 设置值），这可能会使得各个内存区域的总和大于物理内存限制，从而导致动态扩展时出现 OOM。\n \n\n\n## HotSpot 虚拟机堆中的对象\n----------\n\n\n这一小节对JVM对Java堆中的对象的创建、布局和访问的全过程详解。\n\n\n### 对象的创建（遇到一条new指令时）\n![对象的创建](https://i.loli.net/2019/04/09/5cac9255d876b.png)\n\n------------\n1.检查这个指令的参数能否在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，先把这个类加载进内存；\n2.类加载检查通过后，虚拟机将为新对象分配内存，此时已经可以确定存储这个对象所需的内存大小；\n3.在堆中为新对象分配可用内存；\n4.将分配到的内存初始化；\n5.设置对象头中的数据；\n6.此时，从虚拟机的角度看，对象已经创建好了，但从Java程序的角度看，对象创建才刚刚开始，构造函数还没有执行；\n7.执行构造函数。\n第 3 步，在堆中为新对象分配可用内存时，会涉及到以下两个问题：\n\n**如何在堆中为新对象划分可用的内存？**\n\n- 指针碰撞（内存分配规整）\n - 用过的内存放一边，没用过的内存放一边，中间用一个指针分隔；\n - 分配内存的过程就是将指针向没用过的内存那边移动所需的长度；\n- 空闲列表（内存分配不规整）\n - 维护一个列表，记录哪些内存块是可用的；\n- 分配内存时，从列表上选取一块足够大的空间分给对象，并更新列表上的记录；\n\n\n\n**如何处理多线程创建对象时，划分内存的指针的同步问题？**\n\n- 对分配内存空间的动作进行同步处理（CAS）；\n- 把内存分配动作按照线程划分在不同的空间之中进行；\n - 每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer，TLAB）；\n - 哪个线程要分配内存就在哪个线程的 TLAB 上分配，TLAB 用完需要分配新的 TLAB 时，才需要同步锁定；\n - 通过 -XX:+/-UseTLAB 参数设定是否使用 TLAB。\n\n\n### 对象的内存布局\n\n- 对象头：\n - 第一部分：存储对象自身运行时的数据，HashCode、GC分代年龄等（Mark Word）、锁状态标志、、偏向线程ID；\n - 第二部分：类型指针，指向它的类元数据的指针，虚拟机通过这个指针来判断这个对象是哪个类的实例（HotSpot 采用的是直接指针的方式访问对象的），当然类型指针并不一定都在虚拟机中，当有的虚拟机使用句柄来进行对象访问时，此时指针在句柄池中；\n - 如果是个数组对象，对象头中还有一块用于记录数组长度的数据。\n- 实例数据： \n - 默认分配顺序：longs/doubles、ints、shorts/chars、bytes/booleans、oops (Ordinary Object Pointers)，相同宽度的字段会被分配在一起，除了 oops，其他的长度由长到短；\n - 默认分配顺序下，父类字段会被分配在子类字段前面。\n\n注：HotSpot VM要求对象的起始地址必须是8字节的整数倍，所以不够要补齐。\n\n\n\n### 对象访问\n\nJava 程序需要通过虚拟机栈上的 reference 数据来操作堆上的具体对象，reference 数据是一个指向对象的引用，不过如何通过这个引用定位到具体的对象，目前主要有以下两种访问方式：句柄访问和直接指针访问。\n\n\n\n#### 句柄访问\n句柄访问会在 Java 堆中划分一块内存作为句柄池，每一个句柄存放着到对象实例数据和对象类型数据的指针。\n\n优势：对象移动的时候（这在垃圾回收时十分常见）只需改变句柄池中对象实例数据的指针，不需要修改reference本身。\n\n![句柄访问](https://i.loli.net/2019/04/09/5cac96df545e5.png)\n\n\n\n#### 直接指针访问\n\n直接指针访问方式在 Java 堆对象的实例数据中存放了一个指向对象类型数据的指针，在 HotSpot 中，这个指针会被存放在对象头中。\n\n优势：减少了一次指针定位对象实例数据的开销，速度更快。\n\n\n![直接访问](https://i.loli.net/2019/04/09/5cac98512434b.png)\n ","tags":["JVM"],"categories":["JVM"]},{"title":"大数据学习 | 初识 Hadoop","url":"%2F2018%2F12%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0-%E5%88%9D%E8%AF%86Hadoop.html","content":"\n最近想要了解一些前沿技术，不能一门心思眼中只有 web，因为我目前对 Java 语言及其生态相对熟悉，所以在网上搜集了 Hadoop 相关文章，并做了整合。\n\n本篇文章在于对大数据以及 Hadoop 有一个直观的概念，并上手简单体验。\n\n\n<!-- more -->\n\n## Hadoop 基础概念\n\n`Hadoop` 是一个用 Java 实现的开源框架，是一个分布式的解决方案，将大量的信息处理所带来的压力分摊到其他服务器上。\n\n在了解各个名词之前，我们必须掌握一组概念。\n\n### 结构化数据 vs 非结构化数据\n\n`结构化数据`即行数据，存储在数据库里，可以用二维表结构来表达，例如：名字、电话、家庭住址等。\n\n常见的结构化数据库为 mysql、sqlserver。\n\n![zhhihu1.jpg](https://i.loli.net/2018/12/30/5c287655d4f10.jpg)\n\n`非结构化数据库`是指其字段长度可变，并且每个字段的记录又可以由可重复或不可重复的子字段构成的数据库。无法用结构化的数据模型表示，例如：文档、图片、声音、视频等。在大数据时代，对非关系型数据库的需求日益增加，数据库技术相应地进入了“后关系数据库时代”。\n\n非结构化数据库代表为 HBase、mongodb。\n\n![v2-27e5113596ab21aae1d64516ef015100_1200x500.jpg](https://i.loli.net/2018/12/30/5c2876565ece1.jpg)\n\n可以大致归纳，结构化数据是先有结构、再有数据；非结构化数据是先有数据、再有结构。\n\nHadoop 是大数据存储和计算的开山鼻祖，现在大多数开源大数据框架都依赖 Hadoop 或者与它能很好地兼容，下面开始讲述 Hadoop 的相关概念。\n\n### Hadoop 1.0 vs Hadoop 2.0\n\n![Hadoop-1-vs-Hadoop-2-Architecture.png](https://i.loli.net/2018/12/27/5c242519227c9.png)\n\n###  HDFS 和 MapReduce\n\nHadoop 为解决`存储`和`分析`大量数据而生，所以这两部分也是 Hadoop 的狭义说法（广义指 Hadoop 生态）。HDFS 提供了一种安全可靠的分布式文件存储系统，MapReduce 提供了基于批处理模式的数据分析框架。\n\n`HDFS`（Hadoop Distributed File System）的设计本质上是为了大量的数据能横跨很多台机器，但是你看到的是一个文件系统而不是很多个文件系统。就好比访问 `/hdfs/tmp/file1` 的数据，引用的是一个文件路径，但是实际数据可能分布在很多机器上，当然 HDFS 为你管理这些数据，用户并不需要了解它如何管理。\n\n关于 `MapReduce`，这里通过一个具体模型来解释。\n\n考虑如果你要统计一个巨大的文本文件存储在类似 HDFS 上，你想要知道这个文本里各个词的出现频率。你启动了一个 MapReduce 程序。Map 阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似（hello, 12100次），（world，15214次）等等这样的 Pair（我这里把 Map 和 Combine 放在一起说以便简化）；这几百台机器各自都产生了如上的集合，然后又有几百台机器启动 Reduce 处理。Reducer 机器 A 将从 Mapper 机器收到所有以 A 开头的统计结果，机器 B 将收到 B 开头的词汇统计结果（当然实际上不会真的以字母开头做依据，而是用函数产生 Hash 值以避免数据串化。因为类似 X 开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊）。然后这些Reducer将再次汇总，（hello，12100）＋（hello，12311）＋（hello，345881）= （hello，370292）。每个 Reducer 都如上处理，你就得到了整个文件的词频结果。\n\n这就是一个简单的 `WordCount` 的例子，Map+Reduce 这种简单模型暴力好用，不过很笨重，关于更高效的解决方法，以后再详细描述。\n\n###  Hadoop 构建模块\n\n下面从底层实现的角度解释 HDFS 和 MapReduce 的一些概念。\n\n`NameNode` 是 Hadoop 守护进程中最重要的一个。NameNode 位于 HDFS 的主端，指导 DataNode 执行底层的 IO 任务。NameNode 的运行消耗大量内存和 IO 资源，所以 NameNode 服务器不会同时是 DataNode 或 TaskTracker。\n\nNameNode 和 `DataNode` 为主/从结构（Master/Slave）。每一个集群上的从节点都会驻留一个 DataNode 守护进程，来执行分布式文件系统的繁重工作，将 HDFS 数据块读取或者写入到本地文件系统的实际文件中。当希望对 HDFS 文件进行读写时，文件被分割为多个块，由NameNode 告知客户端每个数据块驻留在那个 DataNode。客户端直接与 DataNode 守护进程通信，来处理与数据块相对应的本地文件。\n\n`SNN`（Scondary NameNode）是监测 HDFS 集群状态的辅助守护进程。SNN 快照有助于加少停机的时间并降低数据丢失的风险。\n\n`JobTracker` 守护进程是应用程序和 Hadoop 之间的纽带。一旦提交代码到集群上，JobTracker 就会确定执行计划，包括决定处理哪些文件，为不同的任务分配节点以及监控所有任务的运行。如果任务失败，JobTracker 将自动重启任务，但所分配的节点可能会不同，同时受到预定义的重试次数限制。每一个Hadoop集群只有一个JobTracker守护进程，它通常运行在服务器集群的主节点上。\n\nJobTracker 和 `TaskTracker` 也是主/从结构。JobTracker 作为主节点，监测 MapReduce 作业的整个执行过程，同时，TaskTracker 管理各个任务在每个从节点上的执行情况。TaskTracker 的一个职责就是负责持续不断地与 JobTracker 通讯。如果 JobTracker 在指定的时间内没有收到来自 TaskTracker 的心跳，它会假定 TaskTracker 已经崩溃了，进而重新提交相应的任务到集群的其他节点中。\n\n##  尝试使用 Hadoop\n\n`Hadoop 安装`可以直接看官方文档，或是 Google 一些不错的教程，比如 [Hadoop 的安装](https://chu888chu888.gitbooks.io/hadoopstudy/content/Content/4/chapter0401.html)、[Mac 系统安装Hadoop 2.7.3](https://www.jianshu.com/p/de7eb61c983a)。\n\n按照操作配置 Hadoop 并成功运行，访问`localhost:50070` 和 `localhost:8088` 分别显示一下页面。\n\n![90496E3D-A8FB-41CE-9FF0-3B962184AFAE.png](https://i.loli.net/2018/12/27/5c2457210b58d.png)\n\n![1CBC323A-55DC-40AC-B258-3725DD0D4350.png](https://i.loli.net/2018/12/27/5c2457214a2e2.png)\n\n运行`伪分布式`样例：\n\n![31D3E6A6-5864-4C6E-865E-AE576A64E647.png](https://i.loli.net/2018/12/27/5c24700c97c3a.png)\n\n\n### HDFS 目录/文件操作命令\n\nHDFS 是一种文件系统，它可以将一个很大的数据集存储为一个文件，而大多数其他文件系统无力于这一点。Hadoop 也为它提供了一种与 Linux 命令类似的命令行工具，我们可以进行一些简单的操作。\n\nHadoop 的`文件命令`采取的形式为\n\n```shell\nhadoop fs -cmd <args>\n```\n\n其中 cmd 为具体的文件命令，通常与 UNIX 对应的命令名相同，比如：\n\n```shell\nhadoop fs -ls\nhadoop fs -mkdir /user/seriouszyx\nhadoop fs -lsr /\nhadoop fs -rm example.txt\n```\n\n还有一些本地文件系统和 HDFS 交互的命令，也经常使用到。\n\n```shell\nhadoop fs -put example.txt /user/seriouszyx\nhadoop fs -get example.txt\n```\n\n##  Hadoop 构建模块的原理\n\n### MapReduce 如何分而治之\n\nMapReduce 是用来处理大规模数据的一个并行编程框架，采用了对数据“分而治之”的方法。\n\n![40658-2de7c5066daf7ab1.png](https://i.loli.net/2018/12/30/5c28765631bc8.png)\n\nMapReduce 是一个离线计算框架，它将计算分为两个阶段，Map（并行处理输入数据）和 Reduce（对 Map 结果汇总）。其中 Map 和 Reduce 函数提供了两个高层接口，由用户去编程实现。\n\nMap 的一般处理逻辑为：**(k1;v1) ---->map 处理---->[(k2;v2)]**\n\nReduce 函数的一般处理逻辑是：**(k2;[v2])---->reduce 处理---->[(k3;v3)]**\n\n可以看出 map 处理的输出与 reduce 的输入并不完全相同，这是因为输入参数在进入 reduce 前，一般会将相同键 k2 下的所有值 v2 合并到一个集合中处理：**[(k2;v2)]--->(k2;[v2])**，这个过程叫 Combiner。\n\n在经过 Map 和 Reduce 的抽象后，并行结构模型就变成了下面这样：\n\n![40658-df82b7a1775fac75.png](https://i.loli.net/2018/12/30/5c28765664bab.png)\n\n上图中可以发现，中间有一个同步障（Barrier），其作用是等所有的 map 节点处理完后才进入 reduce，并且这个阶段同时进行数据加工整理过程（Aggregation & Shuffle），以便 reduce 节点可以完全基于本节点上的数据计算最终结果。\n\n不过这仍然不是完整的 MapReduce 模型，在上述框架图中，还少了两个步骤 Combiner 和 Partitioner。\n\n![40658-39cc7b851195657c.png](https://i.loli.net/2018/12/30/5c287656a01e5.png)\n\n\n上述图以`词频统计（WordCount）`为例。\n\n**Combiner** 用来对中间结果数据网络传输进行优化，比如 map 处理完输出很多键值对后，某些键值对的键是相同的，Combiner 就会将相同的键合并，比如有两个键值对的键相同（good，1）和（good，2），便可以合成(good,3)。\n\n这样，可以减少需要传输的中间结果数据量，打倒网络数据传输优化，因为 map 传给 reduce 是通过网络来传的。\n\n**Partitioner** 负责对中间结果进行分区处理。比如词频统计，将所有主键相同的键值对传输给同一个 Reduce 节点，以便 Reduce 节点不需要访问其他 Reduce 节点的情况下，一次性对分过来的中间结果进行处理。\n\n### 副本机制\n\n我们再说回 HDFS 诞生的原因，hdfs 由 Google 最先研发，其需求是单独一台计算机所能存储的空间是有限的，而随着计算机存储空间的加大，其价格是呈几何倍的增长。而 hdfs 架构在相对廉价的计算机上，以分布式的方式，这样想要扩大空间之遥增加集群的数量就可以了。\n\n大量相对廉价的计算机，那么说明**宕机**就是一种必然事件，我们需要让数据避免丢失，就只用采取冗余数据存储，而具体的实现的就是`副本机制`。\n\n![](http://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.png)\n\nhdfs 主要使用`三副本机制`\n\n- 第一副本：如果上传节点是 DN，则上传该节点；如果上传节点是 NN，则随机选择 DN\n- 第二副本：放置在不同机架的 DN 上\n- 第三副本：放置在与第二副本相同机架的不同 DN 上\n\n除了极大程度地避免宕机所造成的数据损失，副本机制还可以在数据读取时进行数据校验。\n\n### NameNode 在做些什么\n\n在 Hadoop 1.0 时代，Hadoop 两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，其中以 NameNode 最为严重。因为 `NameNode 保存了整个 HDFS 的元数据信息`，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。\n\n这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。\n\n所幸的是，在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。\n\n![](https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/img001.png)\n\n从上图中我们可以看到，有两台 NameNode——Active NameNode 和 Standby NameNode，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。\n\n### Yarn\n\n`Yarn` 是 Hadoop 集群的新一代资源管理系统。Hadoop 2.0 对 MapReduce 框架做了彻底的设计重构，我们称 Hadoop 2.0 中的 MapReduce 为 MRv2 或者 Yarn。\n\n![20151029092726524 (2).jpg](https://i.loli.net/2018/12/30/5c28775f7eaa5.jpg)\n\n在 Hadoop 2.x 中，Yarn 把 job 的概念换成了 `application`，因为运行的应用不只是 MapReduce 了，还可能是其他应用，如一个 DAG（有向无环图 Directed Acyclic Graph，例如 Storm 应用）。\n\nYarn 另一个目标是扩展 Hadoop，使得它不仅仅可以支持 MapReduce 计算，还能很方便地管理诸如 Hive、Pig、Hbase、Spark/Shark 等应用。\n\n这种新的架构设计能够使得各种类型的应用运行在 Hadoop 上面，并通过 Yarn 从系统层面进行统一的管理，也就是说，有了 Yarn，**各种应用就可以互不干扰的运行在同一个 Hadoop 系统中**，共享整个集群资源。\n\n\n\n###  ResourceManager 在做些什么\n\n刚刚提到的 Yarn 也采用了 Master/Slave 结构，其中 Master 为 **ResourceManager**，负责整个集群的资源管理与调度；Slave 实现为 **NodeManager**，负责单个节点的组员管理与任务启动。 \n\nResourceManager 是整个 Yarn 集群中最重要的组件之一，它的功能较多，包括 ApplicationMaster 管理（启动、停止等）、NodeManager 管理、Application 管理、状态机管理等。\n\nResourceManager 主要完成以下几个功能：\n- 与客户端交互，处理来自客户端的请求\n- 启动和管理 ApplicationMaster，并在它失败时重新启动它\n- 管理 NodeManager，接受来自 NodeManager 的资源管理汇报信息，并向 NodeManager 下达管理命令或把信息按照一定的策略分配给各个应用程序（ApplicationManager）等\n- **资源管理与调度，接受来自 ApplicationMaster 的资源申请请求，并为之分配资源（核心）**\n\n在 Master/Slave 架构中，ResourceManager 同样存在单点故障（高可用问题，High Availability）问题。为了解决它，通常采用热备方案，即集群中存在一个对外服务的 Active Master 和若干个处于就绪状态的 Standy Master，一旦 Active Master 出现故\n障，立即采用一定的侧率选取某个 Standy Master 转换为 Active Master 以正常对外提供服务。\n\n##  总结\n\n本文介绍了 Hadoop 的相关概念，包括量大核心部件 HDFS 和 MapReduce，并对其进行了进一步剖析，Hadoop 2.0 的 Yarn 的简单介绍，以及一些问题的解决方法（如 HA）。\n\n也通过配置第一次在本机上配置了 Hadoop 的运行环境，运行了伪分布式样例。\n\n接下来会结合一个具体问题深入理解 Hadoop 的方方面面。\n\n<br />\n\n>   References:\n>    [大数据学习笔记](https://chu888chu888.gitbooks.io/hadoopstudy/content/)\n>    [一文读懂大数据平台——写给大数据开发初学者的话!](https://zhuanlan.zhihu.com/p/26545566)\n>   [Hadoop HDFS和MapReduce](https://www.jianshu.com/p/ed6b35f52e3c)\n>   [HDFS文件操作](http://pangjiuzala.github.io/2015/08/03/HDFS%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/)\n>   [hadoop笔记4--MapReduce框架](https://www.jianshu.com/p/35be7bdca902)\n>   [Hadoop Yarn详解](https://blog.csdn.net/suifeng3051/article/details/49486927)\n>   [Hadoop NameNode 高可用 (High Availability) 实现解析](https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html)\n> [Hadoop -YARN ResourceManager 剖析](https://blog.csdn.net/zhangzhebjut/article/details/37730065)\n\n<hr />\n","tags":["Hadoop"],"categories":["知识总结"]},{"title":"[总结|展望] 世界不会因为你的无知而停下脚步","url":"%2F2018%2F12%2F10%2F123.html","content":"\n**Be the greatest, or nothing**。\n","tags":["总结"],"categories":["人生苦旅"]}]